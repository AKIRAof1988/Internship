{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36048850",
   "metadata": {},
   "source": [
    "# 1) Write a python program to display all the header tags from wikipedia.org and make data frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e3ae54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Tag Level                       Tag Text\n",
      "0         h1                      Main Page\n",
      "1         h1           Welcome to Wikipedia\n",
      "2         h2  From today's featured article\n",
      "3         h2               Did you know ...\n",
      "4         h2                    In the news\n",
      "5         h2                    On this day\n",
      "6         h2     From today's featured list\n",
      "7         h2       Today's featured picture\n",
      "8         h2       Other areas of Wikipedia\n",
      "9         h2    Wikipedia's sister projects\n",
      "10        h2            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the Wikipedia page\n",
    "url = \"https://en.wikipedia.org/wiki/Main_Page\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "html_content = response.content\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find all header tags (h1 to h6)\n",
    "header_tags = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "\n",
    "# Extract header tag text and level\n",
    "header_data = []\n",
    "for tag in header_tags:\n",
    "    tag_text = tag.get_text()\n",
    "    tag_level = tag.name\n",
    "    header_data.append({'Tag Level': tag_level, 'Tag Text': tag_text})\n",
    "\n",
    "# Create a DataFrame using pandas\n",
    "header_df = pd.DataFrame(header_data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(header_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b1e2bc",
   "metadata": {},
   "source": [
    "# 2)Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b067a77d",
   "metadata": {},
   "source": [
    "# a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "032179e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ranking              Team Matches Points Rating\n",
      "0       1    Australia\\nAUS      23  2,714    118\n",
      "1       2     Pakistan\\nPAK      20  2,316    116\n",
      "2       3        India\\nIND      36  4,081    113\n",
      "3       4   New Zealand\\nNZ      27  2,806    104\n",
      "4       5      England\\nENG      24  2,426    101\n",
      "5       6  South Africa\\nSA      19  1,910    101\n",
      "6       7   Bangladesh\\nBAN      28  2,661     95\n",
      "7       8  Afghanistan\\nAFG      16  1,404     88\n",
      "8       9     Sri Lanka\\nSL      32  2,794     87\n",
      "9      10   West Indies\\nWI      38  2,582     68\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the ICC cricket rankings page for men's ODI teams\n",
    "url = \"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "html_content = response.content\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find the table containing the rankings\n",
    "table = soup.find('table', class_='table')\n",
    "\n",
    "# Extract team data (Ranking, Team, Matches, Points, Rating) for top 10 teams\n",
    "top_10_teams = []\n",
    "\n",
    "for row in table.find_all('tr')[1:11]:  # Start from index 1 to skip header row, end at index 11 to get top 10\n",
    "    columns = row.find_all('td')\n",
    "    ranking = columns[0].text.strip()\n",
    "    team = columns[1].text.strip()\n",
    "    matches = columns[2].text.strip()\n",
    "    points = columns[3].text.strip()\n",
    "    rating = columns[4].text.strip()\n",
    "    top_10_teams.append({'Ranking': ranking, 'Team': team, 'Matches': matches, 'Points': points, 'Rating': rating})\n",
    "\n",
    "# Create a DataFrame using pandas\n",
    "df = pd.DataFrame(top_10_teams)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a7c083",
   "metadata": {},
   "source": [
    "# b) Top 10 ODI Batsmen along with the records of their team andrating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "649e0b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top ODI Batsman:\n",
      "Name: Babar Azam\n",
      "Team: PAK\n",
      "Rating: 886\n",
      "\n",
      "2) Rassie van der Dussen     Team: SA    Rating: 777\n",
      "3) Fakhar Zaman              Team: PAK   Rating: 755\n",
      "4) Imam-ul-Haq               Team: PAK   Rating: 745\n",
      "5) Shubman Gill              Team: IND   Rating: 743\n",
      "6) Harry Tector              Team: IRE   Rating: 726\n",
      "7) David Warner              Team: AUS   Rating: 726\n",
      "8) Quinton de Kock           Team: SA    Rating: 718\n",
      "9) Virat Kohli               Team: IND   Rating: 705\n",
      "10) Steve Smith               Team: AUS   Rating: 702\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi\"\n",
    "response = requests.get(url)\n",
    "html_content = response.content\n",
    "\n",
    "# Create a BeautifulSoup object\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find the element with class \"rankings-block__top-player\"\n",
    "top_player_element = soup.find('div', class_='rankings-block__top-player')\n",
    "\n",
    "# Extract the name of the top batsman, team, and rating\n",
    "top_batsman_name = top_player_element.find('div', class_='rankings-block__banner--name').text.strip()\n",
    "top_batsman_team = top_player_element.find('div', class_='rankings-block__banner--nationality').find('div')['class'][1]\n",
    "top_batsman_rating = top_player_element.find('div', class_='rankings-block__banner--rating').text.strip()\n",
    "\n",
    "# Print the details of the top-ranked batsman\n",
    "print(f\"Top ODI Batsman:\")\n",
    "print(f\"Name: {top_batsman_name}\")\n",
    "print(f\"Team: {top_batsman_team}\")\n",
    "print(f\"Rating: {top_batsman_rating}\\n\")\n",
    "\n",
    "# Find the table element with class \"table rankings-card-table\"\n",
    "table_element = soup.find('table', class_='table rankings-card-table')\n",
    "\n",
    "# Find all rows (table rows) within the table body\n",
    "rows = table_element.find_all('tr')\n",
    "\n",
    "# Initialize a list to store the top 10 batsmen details\n",
    "top_10_batsmen_details = []\n",
    "\n",
    "# Loop through the rows (excluding the header row)\n",
    "for row in rows[1:]:  # Skip the first row as it contains the header\n",
    "    cols = row.find_all('td')\n",
    "    player_name = cols[1].find('a').text.strip()\n",
    "    player_team = cols[2].find('span', class_='table-body__logo-text').text.strip()\n",
    "    player_rating = cols[3].text.strip()\n",
    "    top_10_batsmen_details.append({'Name': player_name, 'Team': player_team, 'Rating': player_rating})\n",
    "\n",
    "# Print the details of the top 10 ODI batsmen\n",
    "#print(\"Top 10 ODI Batsmen:\")\n",
    "for idx, batsman in enumerate(top_10_batsmen_details, start=2):\n",
    "    print(f\"{idx}) {batsman['Name']: <25} Team: {batsman['Team']: <5} Rating: {batsman['Rating']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8732e984",
   "metadata": {},
   "source": [
    "# c) Top 10 ODI bowlers along with the records of their team andrating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c04c16b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI Bowlers:\n",
      "1) Mitchell Starc            Team: AUS   Rating: 686\n",
      "2) Rashid Khan               Team: AFG   Rating: 682\n",
      "3) Mohammed Siraj            Team: IND   Rating: 670\n",
      "4) Matt Henry                Team: NZ    Rating: 667\n",
      "5) Mujeeb Ur Rahman          Team: AFG   Rating: 661\n",
      "6) Trent Boult               Team: NZ    Rating: 660\n",
      "7) Adam Zampa                Team: AUS   Rating: 652\n",
      "8) Shaheen Afridi            Team: PAK   Rating: 630\n",
      "9) Kuldeep Yadav             Team: IND   Rating: 622\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi\"\n",
    "response = requests.get(url)\n",
    "html_content = response.content\n",
    "\n",
    "# Create a BeautifulSoup object\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find the element with class \"rankings-block__container\" for ODI bowling rankings\n",
    "bowling_rankings_container = soup.find('div', class_='rankings-block__container', attrs={'data-cricket-role': 'bowling', 'data-cricket-scope': 'odi'})\n",
    "\n",
    "# Find all rows within the table body\n",
    "rows = bowling_rankings_container.find_all('tr', class_='table-body')\n",
    "\n",
    "# Initialize a list to store the top 10 bowlers details\n",
    "top_10_bowlers_details = []\n",
    "\n",
    "# Loop through the rows\n",
    "for row in rows:\n",
    "    cols = row.find_all('td')\n",
    "    player_name = cols[1].find('a').text.strip()\n",
    "    player_team = cols[2].find('span', class_='table-body__logo-text').text.strip()\n",
    "    player_rating = cols[3].text.strip()\n",
    "    top_10_bowlers_details.append({'Name': player_name, 'Team': player_team, 'Rating': player_rating})\n",
    "\n",
    "# Print the details of the top 10 ODI bowlers\n",
    "print(\"Top 10 ODI Bowlers:\")\n",
    "for idx, bowler in enumerate(top_10_bowlers_details, start=1):\n",
    "    print(f\"{idx}) {bowler['Name']: <25} Team: {bowler['Team']: <5} Rating: {bowler['Rating']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138005bb",
   "metadata": {},
   "source": [
    "# 3) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c723e127",
   "metadata": {},
   "source": [
    "# a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d201221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Rank          Team Matches Points Rating\n",
      "0    1     Australia      26   4290    165\n",
      "1    2       England      31   3875    125\n",
      "2    3  South Africa      26   3098    119\n",
      "3    4         India      30   3039    101\n",
      "4    5   New Zealand      28   2688     96\n",
      "5    6   West Indies      29   2743     95\n",
      "6    7    Bangladesh      17   1284     76\n",
      "7    8     Sri Lanka      12    820     68\n",
      "8    9      Thailand      13    883     68\n",
      "9   10      Pakistan      27   1678     62\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the ICC Women's ODI team rankings\n",
    "url = \"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the table containing the team rankings\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "\n",
    "# Initialize empty lists to store data\n",
    "rankings = []\n",
    "teams = []\n",
    "matches = []\n",
    "points = []\n",
    "rating = []\n",
    "\n",
    "# Extract data from the table rows\n",
    "rows = table.find_all(\"tr\")[1:11]  # Skip the header row and select top 10 teams\n",
    "for rank, row in enumerate(rows, start=1):\n",
    "    columns = row.find_all(\"td\")\n",
    "    rank_text = columns[0].text.strip()\n",
    "    team_name = columns[1].find(\"span\", class_=\"u-hide-phablet\").text.strip()\n",
    "    matches_text = columns[2].text.strip()\n",
    "    points_text = columns[3].text.strip().replace(\",\", \"\")\n",
    "    rating_text = columns[4].text.strip()\n",
    "    \n",
    "    rankings.append(rank_text)\n",
    "    teams.append(team_name)\n",
    "    matches.append(matches_text)\n",
    "    points.append(points_text)\n",
    "    rating.append(rating_text)\n",
    "\n",
    "# Create a DataFrame\n",
    "data = {\n",
    "    \"Rank\": rankings,\n",
    "    \"Team\": teams,\n",
    "    \"Matches\": matches,\n",
    "    \"Points\": points,\n",
    "    \"Rating\": rating\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d38c3d",
   "metadata": {},
   "source": [
    "# b) Top 10 women’s ODI Batting players along with the records of their team and rating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "eeb0ba0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Player Team  Rating\n",
      "0  Natalie Sciver-Brunt  ENG     803\n",
      "1   Chamari Athapaththu   SL     758\n",
      "2           Beth Mooney  AUS     776\n",
      "3       Laura Wolvaardt   SA     741\n",
      "4       Smriti Mandhana  IND     797\n",
      "5          Alyssa Healy  AUS     785\n",
      "6      Harmanpreet Kaur  IND     731\n",
      "7          Ellyse Perry  AUS     766\n",
      "8           Meg Lanning  AUS     834\n",
      "9       Stafanie Taylor   WI     766\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# URL of the ICC Women's ODI batting rankings\n",
    "url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the top player's name and rating\n",
    "top_player_name = soup.find(\"div\", class_=\"rankings-block__banner--name-large\").text.strip()\n",
    "top_player_rating = int(soup.find(\"div\", class_=\"rankings-block__banner--rating\").text.strip())\n",
    "\n",
    "nationality_element = soup.find(class_='rankings-block__banner--nationality')\n",
    "\n",
    "# Extract the nationality text\n",
    "nationality = nationality_element.get_text(strip=True)[-3:]\n",
    "\n",
    "\n",
    "\n",
    "# Find the table containing the batting player rankings\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "\n",
    "# Initialize empty lists to store data\n",
    "players = [top_player_name]\n",
    "teams = [nationality]\n",
    "ratings = [top_player_rating]\n",
    "\n",
    "# Extract data from the table rows\n",
    "rows = table.find_all(\"tr\")[2:11]  # Skip the header row and select top 10 players\n",
    "for row in rows:\n",
    "    columns = row.find_all(\"td\")\n",
    "    player_name = columns[1].find(\"a\").text.strip()  # Extract player name\n",
    "    players.append(player_name)\n",
    "    teams.append(columns[2].text.strip())\n",
    "    \n",
    "    rating_text = columns[4].text.strip()  # Extract rating text\n",
    "    rating = int(re.search(r'\\d+', rating_text).group()) if re.search(r'\\d+', rating_text) else 0  # Extract numerical part\n",
    "    ratings.append(rating)\n",
    "\n",
    "# Create a DataFrame\n",
    "data = {\n",
    "    \"Player\": players,\n",
    "    \"Team\": teams,\n",
    "    \"Rating\": ratings\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46a8980",
   "metadata": {},
   "source": [
    "# c) Top 10 women’s ODI all-rounder along with the records of their team and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0859f80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Rank                Player Team  Rating\n",
      "0      1  Natalie Sciver-Brunt  ENG     421\n",
      "1      2      Ashleigh Gardner  AUS     389\n",
      "2      3       Hayley Matthews   WI     392\n",
      "3      4        Marizanne Kapp   SA     419\n",
      "4      5          Ellyse Perry  AUS     548\n",
      "5      6           Amelia Kerr   NZ     356\n",
      "6      7         Deepti Sharma  IND     397\n",
      "7      8         Jess Jonassen  AUS     308\n",
      "8      9         Sophie Devine   NZ     305\n",
      "9     10              Nida Dar  PAK     232\n",
      "10    11     Sophie Ecclestone  ENG     217\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# URL of the ICC Women's ODI all-rounder rankings\n",
    "url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the table containing the all-rounder player rankings\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "\n",
    "# Initialize empty lists to store data\n",
    "players = []\n",
    "teams = []\n",
    "ratings = []\n",
    "\n",
    "# Extract top player's information\n",
    "top_player_name = soup.find(\"div\", class_=\"rankings-block__banner--name-large\").text.strip()\n",
    "top_player_team = soup.find(\"div\", class_=\"rankings-block__banner--nationality\").text.strip()\n",
    "top_player_rating = int(soup.find(\"div\", class_=\"rankings-block__banner--rating\").text.strip())\n",
    "\n",
    "# Append top player's information to lists\n",
    "players.append(top_player_name)\n",
    "teams.append(top_player_team)\n",
    "ratings.append(top_player_rating)\n",
    "\n",
    "# Extract data from the table rows\n",
    "rows = table.find_all(\"tr\")[2:12]  # Skip the header row and select rows 2 to 11\n",
    "for rank, row in enumerate(rows, start=1):  # Start rank from 1\n",
    "    columns = row.find_all(\"td\")\n",
    "    player_name = columns[1].find(\"a\").text.strip()  # Extract player name\n",
    "    players.append(player_name)\n",
    "    \n",
    "    team_info = columns[2].find(\"span\", class_=\"table-body__logo-text\")  # Extract team info\n",
    "    team = team_info.get_text(strip=True) if team_info else \"-\"\n",
    "    teams.append(team)\n",
    "    \n",
    "    rating_text = columns[4].text.strip()  # Extract rating text\n",
    "    rating = int(re.search(r'\\d+', rating_text).group()) if re.search(r'\\d+', rating_text) else 0  # Extract numerical part\n",
    "    ratings.append(rating)\n",
    "\n",
    "# Create a DataFrame\n",
    "data = {\n",
    "    \"Rank\": range(1, 12),  # Include rank starting from 1\n",
    "    \"Player\": players,\n",
    "    \"Team\": teams,\n",
    "    \"Rating\": ratings\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea65238",
   "metadata": {},
   "source": [
    "# 4) Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world and make data frame\u0002\n",
    "i) Headline\n",
    "\n",
    "ii) Time\n",
    "\n",
    "iii) News Lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8081bb72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Time</th>\n",
       "      <th>News Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nasdaq falls a fourth day in a row, notches lo...</td>\n",
       "      <td>2023-08-17</td>\n",
       "      <td>https://www.cnbc.comhttps://www.cnbc.com/2023/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can expensive, American-made weapons like F-16...</td>\n",
       "      <td>2023-08-19</td>\n",
       "      <td>https://www.cnbc.comhttps://www.cnbc.com/2023/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Here's why Ukraine wants more big ticket Weste...</td>\n",
       "      <td>2023-08-19</td>\n",
       "      <td>https://www.cnbc.comhttps://www.cnbc.com/video...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U.S. approves shipments of F-16s to Ukraine in...</td>\n",
       "      <td>2023-08-18</td>\n",
       "      <td>https://www.cnbc.comhttps://www.cnbc.com/2023/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ukraine to receive F-16 jets from Denmark and ...</td>\n",
       "      <td>2023-08-18</td>\n",
       "      <td>https://www.cnbc.comhttps://www.cnbc.com/2023/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ukraine sees no hope for F-16s this year; Naft...</td>\n",
       "      <td>2023-08-17</td>\n",
       "      <td>https://www.cnbc.comhttps://www.cnbc.com/2023/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A multitrillion-dollar carbon bubble? Climate ...</td>\n",
       "      <td>2023-08-16</td>\n",
       "      <td>https://www.cnbc.comhttps://www.cnbc.com/2023/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Moving toward low carbon steel is a matter of ...</td>\n",
       "      <td>2023-08-17</td>\n",
       "      <td>https://www.cnbc.comhttps://www.cnbc.com/video...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>We share the same vision as Fortescue Future I...</td>\n",
       "      <td>2023-08-17</td>\n",
       "      <td>https://www.cnbc.comhttps://www.cnbc.com/video...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Business is war, and we go to war every day: G...</td>\n",
       "      <td>2023-08-17</td>\n",
       "      <td>https://www.cnbc.comhttps://www.cnbc.com/video...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Offshore wind turbine catches fire off east co...</td>\n",
       "      <td>2023-08-15</td>\n",
       "      <td>https://www.cnbc.comhttps://www.cnbc.com/2023/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Southeast Asia turns to alternative meats as f...</td>\n",
       "      <td>2023-08-16</td>\n",
       "      <td>https://www.cnbc.comhttps://www.cnbc.com/2023/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Southeast Asia moves closer to economic unity ...</td>\n",
       "      <td>2023-07-30</td>\n",
       "      <td>https://www.cnbc.comhttps://www.cnbc.com/2023/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Indonesian Chamber of Commerce and Industry di...</td>\n",
       "      <td>2023-08-14</td>\n",
       "      <td>https://www.cnbc.comhttps://www.cnbc.com/video...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Stock Exchange of Thailand's president discuss...</td>\n",
       "      <td>2023-08-07</td>\n",
       "      <td>https://www.cnbc.comhttps://www.cnbc.com/video...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Vietnam's EV ownership will see 'strong growth...</td>\n",
       "      <td>2023-08-07</td>\n",
       "      <td>https://www.cnbc.comhttps://www.cnbc.com/2023/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Chances are you haven’t used A.I. to plan a va...</td>\n",
       "      <td>2023-08-14</td>\n",
       "      <td>https://www.cnbc.comhttps://www.cnbc.com/2023/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>$1,850 a day? What it costs to visit the 10 pr...</td>\n",
       "      <td>2023-08-11</td>\n",
       "      <td>https://www.cnbc.comhttps://www.cnbc.com/2023/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Cat-sitting is a whole new travel experience. ...</td>\n",
       "      <td>2023-08-10</td>\n",
       "      <td>https://www.cnbc.comhttps://www.cnbc.com/2023/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>A new luxury hotel is opening near Singapore, ...</td>\n",
       "      <td>2023-08-04</td>\n",
       "      <td>https://www.cnbc.comhttps://www.cnbc.com/2023/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>A market slaughtering dogs was a top tourist a...</td>\n",
       "      <td>2023-08-07</td>\n",
       "      <td>https://www.cnbc.comhttps://www.cnbc.com/2023/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Career choices is the No. 1 conflict among div...</td>\n",
       "      <td>2023-08-19</td>\n",
       "      <td>https://www.cnbc.comhttps://www.cnbc.com/2023/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Harvard gut doctor avoids these 4 foods that c...</td>\n",
       "      <td>2023-08-19</td>\n",
       "      <td>https://www.cnbc.comhttps://www.cnbc.com/2023/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Top 10 best European cities for retirement</td>\n",
       "      <td>2023-08-19</td>\n",
       "      <td>https://www.cnbc.comhttps://www.cnbc.com/2023/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>The No. 1 best state to retire in the U.S.—it'...</td>\n",
       "      <td>2023-08-19</td>\n",
       "      <td>https://www.cnbc.comhttps://www.cnbc.com/2023/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Mark Cuban passed on an Uber investment that c...</td>\n",
       "      <td>2023-08-19</td>\n",
       "      <td>https://www.cnbc.comhttps://www.cnbc.com/2023/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Headline       Time  \\\n",
       "0   Nasdaq falls a fourth day in a row, notches lo... 2023-08-17   \n",
       "1   Can expensive, American-made weapons like F-16... 2023-08-19   \n",
       "2   Here's why Ukraine wants more big ticket Weste... 2023-08-19   \n",
       "3   U.S. approves shipments of F-16s to Ukraine in... 2023-08-18   \n",
       "4   Ukraine to receive F-16 jets from Denmark and ... 2023-08-18   \n",
       "5   Ukraine sees no hope for F-16s this year; Naft... 2023-08-17   \n",
       "6   A multitrillion-dollar carbon bubble? Climate ... 2023-08-16   \n",
       "7   Moving toward low carbon steel is a matter of ... 2023-08-17   \n",
       "8   We share the same vision as Fortescue Future I... 2023-08-17   \n",
       "9   Business is war, and we go to war every day: G... 2023-08-17   \n",
       "10  Offshore wind turbine catches fire off east co... 2023-08-15   \n",
       "11  Southeast Asia turns to alternative meats as f... 2023-08-16   \n",
       "12  Southeast Asia moves closer to economic unity ... 2023-07-30   \n",
       "13  Indonesian Chamber of Commerce and Industry di... 2023-08-14   \n",
       "14  Stock Exchange of Thailand's president discuss... 2023-08-07   \n",
       "15  Vietnam's EV ownership will see 'strong growth... 2023-08-07   \n",
       "16  Chances are you haven’t used A.I. to plan a va... 2023-08-14   \n",
       "17  $1,850 a day? What it costs to visit the 10 pr... 2023-08-11   \n",
       "18  Cat-sitting is a whole new travel experience. ... 2023-08-10   \n",
       "19  A new luxury hotel is opening near Singapore, ... 2023-08-04   \n",
       "20  A market slaughtering dogs was a top tourist a... 2023-08-07   \n",
       "21  Career choices is the No. 1 conflict among div... 2023-08-19   \n",
       "22  Harvard gut doctor avoids these 4 foods that c... 2023-08-19   \n",
       "23         Top 10 best European cities for retirement 2023-08-19   \n",
       "24  The No. 1 best state to retire in the U.S.—it'... 2023-08-19   \n",
       "25  Mark Cuban passed on an Uber investment that c... 2023-08-19   \n",
       "\n",
       "                                            News Link  \n",
       "0   https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n",
       "1   https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n",
       "2   https://www.cnbc.comhttps://www.cnbc.com/video...  \n",
       "3   https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n",
       "4   https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n",
       "5   https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n",
       "6   https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n",
       "7   https://www.cnbc.comhttps://www.cnbc.com/video...  \n",
       "8   https://www.cnbc.comhttps://www.cnbc.com/video...  \n",
       "9   https://www.cnbc.comhttps://www.cnbc.com/video...  \n",
       "10  https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n",
       "11  https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n",
       "12  https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n",
       "13  https://www.cnbc.comhttps://www.cnbc.com/video...  \n",
       "14  https://www.cnbc.comhttps://www.cnbc.com/video...  \n",
       "15  https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n",
       "16  https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n",
       "17  https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n",
       "18  https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n",
       "19  https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n",
       "20  https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n",
       "21  https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n",
       "22  https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n",
       "23  https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n",
       "24  https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n",
       "25  https://www.cnbc.comhttps://www.cnbc.com/2023/...  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "url = \"https://www.cnbc.com/world/?region=world\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Initialize lists to store data\n",
    "headlines = []\n",
    "times = []\n",
    "news_links = []\n",
    "\n",
    "# Find all news articles on the page\n",
    "articles = soup.find_all(\"div\", class_=\"Card-titleContainer\")\n",
    "\n",
    "# Extract data from each article\n",
    "for article in articles:\n",
    "    headline = article.find(\"a\").text.strip()\n",
    "    news_link = \"https://www.cnbc.com\" + article.find(\"a\")[\"href\"]\n",
    "\n",
    "    # Extract the date from the href\n",
    "    date_str = news_link.split(\"/\")[-4:-1]\n",
    "    date = \"/\".join(date_str)\n",
    "    \n",
    "    # Convert date string to datetime object\n",
    "    time = datetime.strptime(date, \"%Y/%m/%d\")\n",
    "    \n",
    "    headlines.append(headline)\n",
    "    times.append(time)\n",
    "    news_links.append(news_link)\n",
    "\n",
    "# Create a dataframe\n",
    "data = {\n",
    "    \"Headline\": headlines,\n",
    "    \"Time\": times,\n",
    "    \"News Link\": news_links\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d2e5e3",
   "metadata": {},
   "source": [
    "# 5) Write a python program to scrape the details of most downloaded articles from AI in last 90\n",
    "days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
    "Scrape below mentioned details and make data frame\n",
    "\n",
    "i) Paper Title\n",
    "\n",
    "ii) Authors\n",
    "\n",
    "iii) Published Date\n",
    "\n",
    "iv) Paper URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "41b8de0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paper Title</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Published Date</th>\n",
       "      <th>Paper URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reward is enough</td>\n",
       "      <td>David Silver, Satinder Singh, Doina Precup, Ri...</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Explanation in artificial intelligence: Insigh...</td>\n",
       "      <td>Tim Miller</td>\n",
       "      <td>February 2019</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Creativity and artificial intelligence</td>\n",
       "      <td>Margaret A. Boden</td>\n",
       "      <td>August 1998</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Conflict-based search for optimal multi-agent ...</td>\n",
       "      <td>Guni Sharon, Roni Stern, Ariel Felner, Nathan ...</td>\n",
       "      <td>February 2015</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Knowledge graphs as tools for explainable mach...</td>\n",
       "      <td>Ilaria Tiddi, Stefan Schlobach</td>\n",
       "      <td>January 2022</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Law and logic: A review from an argumentation ...</td>\n",
       "      <td>Henry Prakken, Giovanni Sartor</td>\n",
       "      <td>October 2015</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Between MDPs and semi-MDPs: A framework for te...</td>\n",
       "      <td>Richard S. Sutton, Doina Precup, Satinder Singh</td>\n",
       "      <td>August 1999</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Explaining individual predictions when feature...</td>\n",
       "      <td>Kjersti Aas, Martin Jullum, Anders Løland</td>\n",
       "      <td>September 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Multiple object tracking: A literature review</td>\n",
       "      <td>Wenhan Luo, Junliang Xing and 4 more</td>\n",
       "      <td>April 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A survey of inverse reinforcement learning: Ch...</td>\n",
       "      <td>Saurabh Arora, Prashant Doshi</td>\n",
       "      <td>August 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Evaluating XAI: A comparison of rule-based and...</td>\n",
       "      <td>Jasper van der Waa, Elisabeth Nieuwburg, Anita...</td>\n",
       "      <td>February 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Explainable AI tools for legal reasoning about...</td>\n",
       "      <td>Joe Collenette, Katie Atkinson, Trevor Bench-C...</td>\n",
       "      <td>April 2023</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Hard choices in artificial intelligence</td>\n",
       "      <td>Roel Dobbe, Thomas Krendl Gilbert, Yonatan Mintz</td>\n",
       "      <td>November 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Assessing the communication gap between AI mod...</td>\n",
       "      <td>Oskar Wysocki, Jessica Katharine Davies and 5 ...</td>\n",
       "      <td>March 2023</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Explaining black-box classifiers using post-ho...</td>\n",
       "      <td>Eoin M. Kenny, Courtney Ford, Molly Quinn, Mar...</td>\n",
       "      <td>May 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The Hanabi challenge: A new frontier for AI re...</td>\n",
       "      <td>Nolan Bard, Jakob N. Foerster and 13 more</td>\n",
       "      <td>March 2020</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Wrappers for feature subset selection</td>\n",
       "      <td>Ron Kohavi, George H. John</td>\n",
       "      <td>December 1997</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Artificial cognition for social human–robot in...</td>\n",
       "      <td>Séverin Lemaignan, Mathieu Warnier and 3 more</td>\n",
       "      <td>June 2017</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>A review of possible effects of cognitive bias...</td>\n",
       "      <td>Tomáš Kliegr, Štěpán Bahník, Johannes Fürnkranz</td>\n",
       "      <td>June 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>The multifaceted impact of Ada Lovelace in the...</td>\n",
       "      <td>Luigia Carlucci Aiello</td>\n",
       "      <td>June 2016</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Robot ethics: Mapping the issues for a mechani...</td>\n",
       "      <td>Patrick Lin, Keith Abney, George Bekey</td>\n",
       "      <td>April 2011</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Reward (Mis)design for autonomous driving</td>\n",
       "      <td>W. Bradley Knox, Alessandro Allievi and 3 more</td>\n",
       "      <td>March 2023</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Planning and acting in partially observable st...</td>\n",
       "      <td>Leslie Pack Kaelbling, Michael L. Littman, Ant...</td>\n",
       "      <td>May 1998</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>What do we want from Explainable Artificial In...</td>\n",
       "      <td>Markus Langer, Daniel Oster and 6 more</td>\n",
       "      <td>July 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Paper Title  \\\n",
       "0                                    Reward is enough   \n",
       "1   Explanation in artificial intelligence: Insigh...   \n",
       "2              Creativity and artificial intelligence   \n",
       "3   Conflict-based search for optimal multi-agent ...   \n",
       "4   Knowledge graphs as tools for explainable mach...   \n",
       "5   Law and logic: A review from an argumentation ...   \n",
       "6   Between MDPs and semi-MDPs: A framework for te...   \n",
       "7   Explaining individual predictions when feature...   \n",
       "8       Multiple object tracking: A literature review   \n",
       "9   A survey of inverse reinforcement learning: Ch...   \n",
       "10  Evaluating XAI: A comparison of rule-based and...   \n",
       "11  Explainable AI tools for legal reasoning about...   \n",
       "12            Hard choices in artificial intelligence   \n",
       "13  Assessing the communication gap between AI mod...   \n",
       "14  Explaining black-box classifiers using post-ho...   \n",
       "15  The Hanabi challenge: A new frontier for AI re...   \n",
       "16              Wrappers for feature subset selection   \n",
       "17  Artificial cognition for social human–robot in...   \n",
       "18  A review of possible effects of cognitive bias...   \n",
       "19  The multifaceted impact of Ada Lovelace in the...   \n",
       "20  Robot ethics: Mapping the issues for a mechani...   \n",
       "21          Reward (Mis)design for autonomous driving   \n",
       "22  Planning and acting in partially observable st...   \n",
       "23  What do we want from Explainable Artificial In...   \n",
       "\n",
       "                                              Authors  Published Date  \\\n",
       "0   David Silver, Satinder Singh, Doina Precup, Ri...    October 2021   \n",
       "1                                          Tim Miller   February 2019   \n",
       "2                                   Margaret A. Boden     August 1998   \n",
       "3   Guni Sharon, Roni Stern, Ariel Felner, Nathan ...   February 2015   \n",
       "4                      Ilaria Tiddi, Stefan Schlobach    January 2022   \n",
       "5                      Henry Prakken, Giovanni Sartor    October 2015   \n",
       "6     Richard S. Sutton, Doina Precup, Satinder Singh     August 1999   \n",
       "7           Kjersti Aas, Martin Jullum, Anders Løland  September 2021   \n",
       "8                Wenhan Luo, Junliang Xing and 4 more      April 2021   \n",
       "9                       Saurabh Arora, Prashant Doshi     August 2021   \n",
       "10  Jasper van der Waa, Elisabeth Nieuwburg, Anita...   February 2021   \n",
       "11  Joe Collenette, Katie Atkinson, Trevor Bench-C...      April 2023   \n",
       "12   Roel Dobbe, Thomas Krendl Gilbert, Yonatan Mintz   November 2021   \n",
       "13  Oskar Wysocki, Jessica Katharine Davies and 5 ...      March 2023   \n",
       "14  Eoin M. Kenny, Courtney Ford, Molly Quinn, Mar...        May 2021   \n",
       "15          Nolan Bard, Jakob N. Foerster and 13 more      March 2020   \n",
       "16                         Ron Kohavi, George H. John   December 1997   \n",
       "17      Séverin Lemaignan, Mathieu Warnier and 3 more       June 2017   \n",
       "18    Tomáš Kliegr, Štěpán Bahník, Johannes Fürnkranz       June 2021   \n",
       "19                             Luigia Carlucci Aiello       June 2016   \n",
       "20             Patrick Lin, Keith Abney, George Bekey      April 2011   \n",
       "21     W. Bradley Knox, Alessandro Allievi and 3 more      March 2023   \n",
       "22  Leslie Pack Kaelbling, Michael L. Littman, Ant...        May 1998   \n",
       "23             Markus Langer, Daniel Oster and 6 more       July 2021   \n",
       "\n",
       "                                            Paper URL  \n",
       "0   https://www.sciencedirect.com/science/article/...  \n",
       "1   https://www.sciencedirect.com/science/article/...  \n",
       "2   https://www.sciencedirect.com/science/article/...  \n",
       "3   https://www.sciencedirect.com/science/article/...  \n",
       "4   https://www.sciencedirect.com/science/article/...  \n",
       "5   https://www.sciencedirect.com/science/article/...  \n",
       "6   https://www.sciencedirect.com/science/article/...  \n",
       "7   https://www.sciencedirect.com/science/article/...  \n",
       "8   https://www.sciencedirect.com/science/article/...  \n",
       "9   https://www.sciencedirect.com/science/article/...  \n",
       "10  https://www.sciencedirect.com/science/article/...  \n",
       "11  https://www.sciencedirect.com/science/article/...  \n",
       "12  https://www.sciencedirect.com/science/article/...  \n",
       "13  https://www.sciencedirect.com/science/article/...  \n",
       "14  https://www.sciencedirect.com/science/article/...  \n",
       "15  https://www.sciencedirect.com/science/article/...  \n",
       "16  https://www.sciencedirect.com/science/article/...  \n",
       "17  https://www.sciencedirect.com/science/article/...  \n",
       "18  https://www.sciencedirect.com/science/article/...  \n",
       "19  https://www.sciencedirect.com/science/article/...  \n",
       "20  https://www.sciencedirect.com/science/article/...  \n",
       "21  https://www.sciencedirect.com/science/article/...  \n",
       "22  https://www.sciencedirect.com/science/article/...  \n",
       "23  https://www.sciencedirect.com/science/article/...  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Initialize lists to store data\n",
    "paper_titles = []\n",
    "authors_list = []\n",
    "published_dates = []\n",
    "paper_urls = []\n",
    "\n",
    "# Find all article items on the page\n",
    "article_items = soup.find_all(\"article\")\n",
    "\n",
    "# Extract data from each article item\n",
    "for item in article_items:\n",
    "    # Extract paper title\n",
    "    paper_title = item.find(\"h2\").text.strip()\n",
    "    paper_titles.append(paper_title)\n",
    "    \n",
    "    # Extract authors\n",
    "    authors = item.find(\"span\", class_=\"sc-1w3fpd7-0\").text.strip()\n",
    "    authors_list.append(authors)\n",
    "    \n",
    "    # Extract published date\n",
    "    published_date = item.find(\"span\", class_=\"sc-1thf9ly-2\").text.strip()\n",
    "    published_dates.append(published_date)\n",
    "    \n",
    "    # Extract paper URL\n",
    "    paper_url = item.find(\"a\", class_=\"sc-5smygv-0\")[\"href\"]\n",
    "    paper_urls.append(paper_url)\n",
    "\n",
    "# Create a dataframe\n",
    "data = {\n",
    "    \"Paper Title\": paper_titles,\n",
    "    \"Authors\": authors_list,\n",
    "    \"Published Date\": published_dates,\n",
    "    \"Paper URL\": paper_urls\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9355aa0e",
   "metadata": {},
   "source": [
    "# 6) Write a python program to scrape mentioned details from dineout.co.inand make data frame \n",
    "i) Restaurant name\n",
    "\n",
    "ii) Cuisine\n",
    "\n",
    "iii) Location\n",
    "\n",
    "iv) Ratings\n",
    "\n",
    "v) Image UR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5030ced1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Restaurant Name Cuisine Location Ratings  \\\n",
      "0                       Station Bar                        4   \n",
      "1                             Local                        4   \n",
      "2                    Openhouse Cafe                      4.1   \n",
      "3                           Tamasha                      4.2   \n",
      "4                     My Bar Square                      3.9   \n",
      "5                     The G.T. Road                      4.3   \n",
      "6                  Ministry Of Beer                        4   \n",
      "7                    Warehouse Cafe                      4.1   \n",
      "8              Connaught Club House                      4.2   \n",
      "9                 The Junkyard Cafe                      4.1   \n",
      "10              Unplugged Courtyard                        4   \n",
      "11               Lord of the Drinks                      4.2   \n",
      "12                          Berco's                      4.3   \n",
      "13                      Dasaprakash                      4.2   \n",
      "14              My Bar Headquarters                        4   \n",
      "15                        Oh My God                        4   \n",
      "16                           Sandoz                        4   \n",
      "17                              QBA                      4.2   \n",
      "18                      38 Barracks                      4.3   \n",
      "19  Ardor 2.1 Restaurant and Lounge                      4.1   \n",
      "20                   Lazeez Affaire                      4.1   \n",
      "\n",
      "                                            Image URL  \n",
      "0   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "1   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "2   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "3   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "4   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "5   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "6   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "7   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "8   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "9   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "10  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "11  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "12  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "13  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "14  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "15  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "16  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "17  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "18  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "19  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "20  https://im1.dineout.co.in/images/uploads/resta...  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the page to scrape\n",
    "url = \"https://www.dineout.co.in/delhi-restaurants\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all restaurant cards\n",
    "restaurant_cards = soup.find_all('div', class_='restnt-card')\n",
    "\n",
    "# Lists to store scraped data\n",
    "restaurant_names = []\n",
    "cuisines = []\n",
    "locations = []\n",
    "ratings = []\n",
    "image_urls = []\n",
    "\n",
    "# Loop through each restaurant card and extract the required details\n",
    "for card in restaurant_cards:\n",
    "    restaurant_name = card.find('a', class_='restnt-name').text.strip()\n",
    "    cuisine_tags = card.find_all('a', href=True, data_w_onclick=True)\n",
    "    cuisine_list = [cuisine.text for cuisine in cuisine_tags]\n",
    "    cuisine = ', '.join(cuisine_list)\n",
    "    location_tags = card.find_all('a', href=True, data_name=True, data_type=True)\n",
    "    location_list = [location['data-name'] for location in location_tags if location['data-type'] == 'LocalityClick']\n",
    "    location = ', '.join(location_list)\n",
    "    rating = card.find('div', class_='restnt-rating').text.strip()\n",
    "    image_url = card.find('img', class_='no-img')['data-src']\n",
    "\n",
    "    restaurant_names.append(restaurant_name)\n",
    "    cuisines.append(cuisine)\n",
    "    locations.append(location)\n",
    "    ratings.append(rating)\n",
    "    image_urls.append(image_url)\n",
    "\n",
    "# Create a DataFrame from the scraped data\n",
    "data = {\n",
    "    'Restaurant Name': restaurant_names,\n",
    "    'Cuisine': cuisines,\n",
    "    'Location': locations,\n",
    "    'Ratings': ratings,\n",
    "    'Image URL': image_urls\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8c61ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant Name</th>\n",
       "      <th>Cuisine</th>\n",
       "      <th>Location</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Image URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Station Bar</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Local</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Openhouse Cafe</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4.1</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tamasha</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4.2</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My Bar Square</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The G.T. Road</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ministry Of Beer</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Warehouse Cafe</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4.1</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Connaught Club House</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4.2</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Junkyard Cafe</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4.1</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Unplugged Courtyard</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Lord of the Drinks</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4.2</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Berco's</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Dasaprakash</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4.2</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>My Bar Headquarters</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Oh My God</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Sandoz</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>QBA</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4.2</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>38 Barracks</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Ardor 2.1 Restaurant and Lounge</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4.1</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Lazeez Affaire</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4.1</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Restaurant Name Cuisine Location Ratings  \\\n",
       "0                       Station Bar                        4   \n",
       "1                             Local                        4   \n",
       "2                    Openhouse Cafe                      4.1   \n",
       "3                           Tamasha                      4.2   \n",
       "4                     My Bar Square                      3.9   \n",
       "5                     The G.T. Road                      4.3   \n",
       "6                  Ministry Of Beer                        4   \n",
       "7                    Warehouse Cafe                      4.1   \n",
       "8              Connaught Club House                      4.2   \n",
       "9                 The Junkyard Cafe                      4.1   \n",
       "10              Unplugged Courtyard                        4   \n",
       "11               Lord of the Drinks                      4.2   \n",
       "12                          Berco's                      4.3   \n",
       "13                      Dasaprakash                      4.2   \n",
       "14              My Bar Headquarters                        4   \n",
       "15                        Oh My God                        4   \n",
       "16                           Sandoz                        4   \n",
       "17                              QBA                      4.2   \n",
       "18                      38 Barracks                      4.3   \n",
       "19  Ardor 2.1 Restaurant and Lounge                      4.1   \n",
       "20                   Lazeez Affaire                      4.1   \n",
       "\n",
       "                                            Image URL  \n",
       "0   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "1   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "2   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "3   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "4   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "5   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "6   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "7   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "8   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "9   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "10  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "11  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "12  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "13  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "14  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "15  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "16  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "17  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "18  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "19  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "20  https://im1.dineout.co.in/images/uploads/resta...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a486cb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the page to scrape\n",
    "url = \"https://www.dineout.co.in/delhi-restaurants\"\n",
    "base_url=\"https://www.dineout.co.in\"\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all restaurant cards\n",
    "restaurant_cards = soup.find_all('a', class_=\"restnt-name ellipsis\")\n",
    "\n",
    "for card in restaurant_cards:\n",
    "    # Extract the restaurant name\n",
    "    restaurant_name = card.get_text()\n",
    "\n",
    "    # Extract the href attribute\n",
    "    href = card['href']\n",
    "    restaurant_info= request.get(base_url+href)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2585acce",
   "metadata": {},
   "source": [
    "<div class=\"restnt-details_info\"><h1>Castle Barbeque</h1><div class=\"restnt-cost\">₹ 2,000 for 2 | <a href=\"/delhi-restaurants/central-delhi/connaught-place/chinese-cuisine\" class=\"text-ltgrey\">Chinese</a>, <a href=\"/delhi-restaurants/central-delhi/connaught-place/north-indian-cuisine\" class=\"text-ltgrey\">North Indian</a></div><div class=\"restnt-name\"><a href=\"/delhi-restaurants/central-delhi/connaught-place\" class=\"text-ltgrey\">Connaught Place | </a><a href=\"/delhi-restaurants\" class=\"text-ltgrey\">Delhi | </a><a href=\"javascript:void(0)\" data-toggle=\"modal\" data-target=\"#direction\" class=\"text-blue font-bold\" item-clicked=\"direction\" data-w-onclick=\"restroNavClicked|w1\"><img src=\"https://im1.dineout.co.in/images/uploads/mailer/2019/Jul/11/path.png\" width=\"13\" height=\"13\"> Get Direction</a></div><div class=\"timing\">Time: <a href=\"javascript:void(0)\" class=\"dropdown-toggle\" id=\"dropdownMenu1\" data-toggle=\"dropdown\" data-action=\"rdp-timings\" aria-expanded=\"true\" data-w-onclick=\"restroGAEvents|w1\"><span class=\"text-blue font-bold\"> (Opens at 12:00 PM)</span><i class=\"do do-angle-down\"></i><div class=\"opening-hrs-wrap\"><div class=\"time-wrap\"><div class=\"all-timings\"><div class=\"tooltip bottom\" role=\"menu\" aria-labelledby=\"dropdownMenu1\"><div class=\"tooltip-arrow\"></div><div class=\"tooltip-inner\"><ul><li><span class=\"day\">Sunday</span>12:00 PM to 12:00 AM</li><li><span class=\"day\">Monday</span>12:00 PM to 12:00 AM</li><li><span class=\"day\">Tuesday</span>12:00 PM to 12:00 AM</li><li><span class=\"day\">Wednesday</span>12:00 PM to 12:00 AM</li><li><span class=\"day\">Thursday</span>12:00 PM to 12:00 AM</li><li><span class=\"day\">Friday</span>12:00 PM to 12:00 AM</li><li><span class=\"day\">Saturday</span>12:00 PM to 12:00 AM</li></ul></div></div></div></div></div></a></div></div>\n",
    "\n",
    "from this reference extract cuisine,location \n",
    "from restaurant_cards = soup.find_all('a', class_=\"restnt-name ellipsis\") extract Restaurant name,Cuisine,Image URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2e1d1551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.dineout.co.in/delhi/station-bar-connaught-place-central-delhi-971\n"
     ]
    }
   ],
   "source": [
    "print(\"https://www.dineout.co.in\"+\"/delhi/station-bar-connaught-place-central-delhi-971\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "455c7b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Restaurant Name  \\\n",
      "0                       Station Bar   \n",
      "1                             Local   \n",
      "2                    Openhouse Cafe   \n",
      "3                           Tamasha   \n",
      "4                     My Bar Square   \n",
      "5                     The G.T. Road   \n",
      "6                  Ministry Of Beer   \n",
      "7                    Warehouse Cafe   \n",
      "8              Connaught Club House   \n",
      "9                 The Junkyard Cafe   \n",
      "10              Unplugged Courtyard   \n",
      "11               Lord of the Drinks   \n",
      "12                          Berco's   \n",
      "13                      Dasaprakash   \n",
      "14              My Bar Headquarters   \n",
      "15                        Oh My God   \n",
      "16                           Sandoz   \n",
      "17                              QBA   \n",
      "18                      38 Barracks   \n",
      "19  Ardor 2.1 Restaurant and Lounge   \n",
      "20                   Lazeez Affaire   \n",
      "\n",
      "                                              Cuisine  \\\n",
      "0   ₹ 1,100 for 2 |Italian,Chinese,North Indian,Fa...   \n",
      "1       ₹ 2,000 for 2 |North Indian,Asian,Continental   \n",
      "2           ₹ 2,000 for 2 |North Indian,Asian,Italian   \n",
      "3   ₹ 2,000 for 2 |Continental,Asian,Italian,North...   \n",
      "4   ₹ 2,000 for 2 |Finger Food,Chinese,Continental...   \n",
      "5                         ₹ 2,000 for 2 |North Indian   \n",
      "6   ₹ 3,000 for 2 |North Indian,Continental,Americ...   \n",
      "7         ₹ 2,500 for 2 |North Indian,Chinese,Italian   \n",
      "8   ₹ 1,800 for 2 |North Indian,Continental,Asian,...   \n",
      "9   ₹ 2,100 for 2 |North Indian,Continental,Chines...   \n",
      "10  ₹ 2,500 for 2 |North Indian,Italian,Chinese,Tu...   \n",
      "11      ₹ 2,500 for 2 |Fast Food,Chinese,North Indian   \n",
      "12                        ₹ 1,300 for 2 |Chinese,Thai   \n",
      "13  ₹ 800 for 2 |Beverages,Chinese,Continental,Nor...   \n",
      "14                ₹ 1,500 for 2 |North Indian,Chinese   \n",
      "15    ₹ 1,100 for 2 |Continental,North Indian,Chinese   \n",
      "16            ₹ 1,400 for 2 |Continental,North Indian   \n",
      "17    ₹ 2,100 for 2 |North Indian,Continental,Italian   \n",
      "18    ₹ 2,700 for 2 |North Indian,Chinese,Continental   \n",
      "19  ₹ 2,000 for 2 |North Indian,Chinese,Italian,Co...   \n",
      "20        ₹ 2,600 for 2 |Mughlai,Chinese,North Indian   \n",
      "\n",
      "                                             Location  \\\n",
      "0      F-Block |Connaught Place |Delhi |Get Direction   \n",
      "1   Scindia House |Connaught Place |Delhi |Get Dir...   \n",
      "2               Connaught Place |Delhi |Get Direction   \n",
      "3               Connaught Place |Delhi |Get Direction   \n",
      "4               Connaught Place |Delhi |Get Direction   \n",
      "5      M-Block |Connaught Place |Delhi |Get Direction   \n",
      "6      M-Block |Connaught Place |Delhi |Get Direction   \n",
      "7               Connaught Place |Delhi |Get Direction   \n",
      "8               Connaught Place |Delhi |Get Direction   \n",
      "9               Connaught Place |Delhi |Get Direction   \n",
      "10              Connaught Place |Delhi |Get Direction   \n",
      "11              Connaught Place |Delhi |Get Direction   \n",
      "12              Connaught Place |Delhi |Get Direction   \n",
      "13              Connaught Place |Delhi |Get Direction   \n",
      "14              Connaught Place |Delhi |Get Direction   \n",
      "15     F-Block |Connaught Place |Delhi |Get Direction   \n",
      "16              Connaught Place |Delhi |Get Direction   \n",
      "17              Connaught Place |Delhi |Get Direction   \n",
      "18     M-Block |Connaught Place |Delhi |Get Direction   \n",
      "19              Connaught Place |Delhi |Get Direction   \n",
      "20              Connaught Place |Delhi |Get Direction   \n",
      "\n",
      "                                            Image URL  \n",
      "0   https://im1.dineout.co.in/images/uploads/maile...  \n",
      "1   https://im1.dineout.co.in/images/uploads/maile...  \n",
      "2   https://im1.dineout.co.in/images/uploads/maile...  \n",
      "3   https://im1.dineout.co.in/images/uploads/maile...  \n",
      "4   https://im1.dineout.co.in/images/uploads/maile...  \n",
      "5   https://im1.dineout.co.in/images/uploads/maile...  \n",
      "6   https://im1.dineout.co.in/images/uploads/maile...  \n",
      "7   https://im1.dineout.co.in/images/uploads/maile...  \n",
      "8   https://im1.dineout.co.in/images/uploads/maile...  \n",
      "9   https://im1.dineout.co.in/images/uploads/maile...  \n",
      "10  https://im1.dineout.co.in/images/uploads/maile...  \n",
      "11  https://im1.dineout.co.in/images/uploads/maile...  \n",
      "12  https://im1.dineout.co.in/images/uploads/maile...  \n",
      "13  https://im1.dineout.co.in/images/uploads/maile...  \n",
      "14  https://im1.dineout.co.in/images/uploads/maile...  \n",
      "15  https://im1.dineout.co.in/images/uploads/maile...  \n",
      "16  https://im1.dineout.co.in/images/uploads/maile...  \n",
      "17  https://im1.dineout.co.in/images/uploads/maile...  \n",
      "18  https://im1.dineout.co.in/images/uploads/maile...  \n",
      "19  https://im1.dineout.co.in/images/uploads/maile...  \n",
      "20  https://im1.dineout.co.in/images/uploads/maile...  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# URL of the page to scrape\n",
    "base_url = \"https://www.dineout.co.in\"\n",
    "url = \"https://www.dineout.co.in/delhi-restaurants\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all restaurant cards\n",
    "restaurant_cards = soup.find_all('a', class_=\"restnt-name ellipsis\")\n",
    "\n",
    "# Create empty lists to store extracted data\n",
    "restaurant_data = []\n",
    "\n",
    "# Iterate through each restaurant card\n",
    "for card in restaurant_cards:\n",
    "    # Extract the restaurant name\n",
    "    restaurant_name = card.get_text()\n",
    "\n",
    "    # Extract the href attribute\n",
    "    href = card['href']\n",
    "\n",
    "    # Construct the full URL\n",
    "    full_url = urljoin(base_url, href)\n",
    "\n",
    "    # Follow the href link to the restaurant details page\n",
    "    restaurant_response = requests.get(full_url)\n",
    "    restaurant_soup = BeautifulSoup(restaurant_response.content, 'html.parser')\n",
    "    \n",
    "    # Extract the div containing restaurant details\n",
    "    details_div = restaurant_soup.find('div', class_=\"restnt-details_info\")\n",
    "\n",
    "    if details_div:\n",
    "        # Extract cuisine and location\n",
    "        cuisine = details_div.find('div', class_=\"restnt-cost\").get_text(strip=True)\n",
    "        location = details_div.find('div', class_=\"restnt-name\").get_text(strip=True)\n",
    "\n",
    "        # Extract the image URL\n",
    "        image_url = details_div.find('a', class_=\"text-blue font-bold\").find('img')['src']\n",
    "\n",
    "        # Append data to the list\n",
    "        restaurant_data.append({\n",
    "            \"Restaurant Name\": restaurant_name,\n",
    "            \"Cuisine\": cuisine,\n",
    "            \"Location\": location,\n",
    "            \"Image URL\": image_url\n",
    "        })\n",
    "\n",
    "# Convert the list to a pandas DataFrame\n",
    "restaurant_df = pd.DataFrame(restaurant_data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(restaurant_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7ed43c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant Name</th>\n",
       "      <th>Cuisine</th>\n",
       "      <th>Location</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Image URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Station Bar</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Local</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Openhouse Cafe</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4.1</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tamasha</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4.2</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My Bar Square</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The G.T. Road</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ministry Of Beer</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Warehouse Cafe</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4.1</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Connaught Club House</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4.2</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Junkyard Cafe</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4.1</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Unplugged Courtyard</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Lord of the Drinks</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4.2</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Berco's</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Dasaprakash</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4.2</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>My Bar Headquarters</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Oh My God</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Sandoz</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>QBA</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4.2</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>38 Barracks</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Ardor 2.1 Restaurant and Lounge</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4.1</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Lazeez Affaire</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4.1</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Restaurant Name Cuisine Location Ratings  \\\n",
       "0                       Station Bar                        4   \n",
       "1                             Local                        4   \n",
       "2                    Openhouse Cafe                      4.1   \n",
       "3                           Tamasha                      4.2   \n",
       "4                     My Bar Square                      3.9   \n",
       "5                     The G.T. Road                      4.3   \n",
       "6                  Ministry Of Beer                        4   \n",
       "7                    Warehouse Cafe                      4.1   \n",
       "8              Connaught Club House                      4.2   \n",
       "9                 The Junkyard Cafe                      4.1   \n",
       "10              Unplugged Courtyard                        4   \n",
       "11               Lord of the Drinks                      4.2   \n",
       "12                          Berco's                      4.3   \n",
       "13                      Dasaprakash                      4.2   \n",
       "14              My Bar Headquarters                        4   \n",
       "15                        Oh My God                        4   \n",
       "16                           Sandoz                        4   \n",
       "17                              QBA                      4.2   \n",
       "18                      38 Barracks                      4.3   \n",
       "19  Ardor 2.1 Restaurant and Lounge                      4.1   \n",
       "20                   Lazeez Affaire                      4.1   \n",
       "\n",
       "                                            Image URL  \n",
       "0   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "1   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "2   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "3   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "4   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "5   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "6   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "7   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "8   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "9   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "10  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "11  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "12  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "13  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "14  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "15  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "16  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "17  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "18  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "19  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "20  https://im1.dineout.co.in/images/uploads/resta...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9415a730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Restaurant Name  \\\n",
      "0                       Station Bar   \n",
      "1                             Local   \n",
      "2                    Openhouse Cafe   \n",
      "3                           Tamasha   \n",
      "4                     My Bar Square   \n",
      "5                     The G.T. Road   \n",
      "6                  Ministry Of Beer   \n",
      "7                    Warehouse Cafe   \n",
      "8              Connaught Club House   \n",
      "9                 The Junkyard Cafe   \n",
      "10              Unplugged Courtyard   \n",
      "11               Lord of the Drinks   \n",
      "12                          Berco's   \n",
      "13                      Dasaprakash   \n",
      "14              My Bar Headquarters   \n",
      "15                        Oh My God   \n",
      "16                           Sandoz   \n",
      "17                              QBA   \n",
      "18                      38 Barracks   \n",
      "19  Ardor 2.1 Restaurant and Lounge   \n",
      "20                   Lazeez Affaire   \n",
      "\n",
      "                                              Cuisine           Location  \\\n",
      "0           Italian, Chinese, North Indian, Fast Food          F-Block |   \n",
      "1                    North Indian, Asian, Continental    Scindia House |   \n",
      "2                        North Indian, Asian, Italian  Connaught Place |   \n",
      "3           Continental, Asian, Italian, North Indian  Connaught Place |   \n",
      "4          Finger Food, Chinese, Continental, Italian  Connaught Place |   \n",
      "5                                        North Indian          M-Block |   \n",
      "6          North Indian, Continental, American, Asian          M-Block |   \n",
      "7                      North Indian, Chinese, Italian  Connaught Place |   \n",
      "8           North Indian, Continental, Asian, Chinese  Connaught Place |   \n",
      "9       North Indian, Continental, Chinese, Fast Food  Connaught Place |   \n",
      "10  North Indian, Italian, Chinese, Turkish, Conti...  Connaught Place |   \n",
      "11                   Fast Food, Chinese, North Indian  Connaught Place |   \n",
      "12                                      Chinese, Thai  Connaught Place |   \n",
      "13  Beverages, Chinese, Continental, North Indian,...  Connaught Place |   \n",
      "14                              North Indian, Chinese  Connaught Place |   \n",
      "15                 Continental, North Indian, Chinese          F-Block |   \n",
      "16                          Continental, North Indian  Connaught Place |   \n",
      "17                 North Indian, Continental, Italian  Connaught Place |   \n",
      "18                 North Indian, Chinese, Continental          M-Block |   \n",
      "19        North Indian, Chinese, Italian, Continental  Connaught Place |   \n",
      "20                     Mughlai, Chinese, North Indian  Connaught Place |   \n",
      "\n",
      "   Ratings Image URL  \n",
      "0                     \n",
      "1                     \n",
      "2                     \n",
      "3                     \n",
      "4                     \n",
      "5                     \n",
      "6                     \n",
      "7                     \n",
      "8                     \n",
      "9                     \n",
      "10                    \n",
      "11                    \n",
      "12                    \n",
      "13                    \n",
      "14                    \n",
      "15                    \n",
      "16                    \n",
      "17                    \n",
      "18                    \n",
      "19                    \n",
      "20                    \n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://www.dineout.co.in\"  # Base URL for the restaurant cards\n",
    "\n",
    "# Initialize empty lists to store data\n",
    "restaurant_names = []\n",
    "cuisines = []\n",
    "locations = []\n",
    "ratings = []\n",
    "image_urls = []\n",
    "\n",
    "# Loop through each restaurant card\n",
    "for card in restaurant_cards:\n",
    "    # Extract restaurant name\n",
    "    restaurant_name = card.text.strip()\n",
    "    restaurant_names.append(restaurant_name)\n",
    "    \n",
    "    # Combine base URL with href to create complete URL\n",
    "    href = card['href']\n",
    "    complete_url = base_url + href\n",
    "    \n",
    "    # Send a GET request to the complete URL\n",
    "    restaurant_response = requests.get(complete_url)\n",
    "    restaurant_soup = BeautifulSoup(restaurant_response.content, 'html.parser')\n",
    "    \n",
    "    # Extract the div containing restaurant details\n",
    "    details_div = restaurant_soup.find('div', class_='restnt-details_info')\n",
    "    \n",
    "    # Extract cuisine and location\n",
    "    cuisine_element = details_div.find('div', class_='restnt-cost')\n",
    "    cuisine = ', '.join([a.text for a in cuisine_element.find_all('a') if a.has_attr('href')])\n",
    "    cuisines.append(cuisine)\n",
    "    \n",
    "    location_element = details_div.find('div', class_='restnt-name')\n",
    "    location = location_element.find('a', class_='text-ltgrey').text.strip()\n",
    "    locations.append(location)\n",
    "    \n",
    "    # Extract the ratings and image URL\n",
    "    rating_element = details_div.find('div', class_='cursor rest-rating')\n",
    "    rating = rating_element.text.strip() if rating_element else ''\n",
    "    ratings.append(rating)\n",
    "    \n",
    "    image_element = restaurant_soup.find('img', class_='rest-image')\n",
    "    image_url = image_element['src'] if image_element else ''\n",
    "    image_urls.append(image_url)\n",
    "\n",
    "# Create a DataFrame\n",
    "data = {\n",
    "    \"Restaurant Name\": restaurant_names,\n",
    "    \"Cuisine\": cuisines,\n",
    "    \"Location\": locations,\n",
    "    \"Ratings\": ratings,\n",
    "    \"Image URL\": image_urls\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2392377",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
